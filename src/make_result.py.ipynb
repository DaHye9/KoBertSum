{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Korean Sentence Splitter]: Initializing Kss...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "#import MeCab\n",
    "from bs4 import BeautifulSoup\n",
    "import kss\n",
    "import json\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import pickle\n",
    "\n",
    "PROBLEM = 'ext'\n",
    "\n",
    "## 사용할 path 정의\n",
    "# PROJECT_DIR = '/home/uoneway/Project/PreSumm_ko'\n",
    "PROJECT_DIR = '/home/ubuntu/KoBertSum'\n",
    "\n",
    "DATA_DIR = f'{PROJECT_DIR}/{PROBLEM}/data'\n",
    "RAW_DATA_DIR1 = DATA_DIR + '/kmooc_test'\n",
    "RAW_DATA_DIR2 = DATA_DIR + '/raw'\n",
    "JSON_DATA_DIR = DATA_DIR + '/json_data'\n",
    "BERT_DATA_DIR = DATA_DIR + '/bert_data' \n",
    "LOG_DIR = f'{PROJECT_DIR}/{PROBLEM}/logs'\n",
    "LOG_PREPO_FILE = LOG_DIR + '/preprocessing.log' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['-file'], dest='file', nargs=None, const=None, default=None, type=<class 'str'>, choices=None, help=None, metavar=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"-task\", default=None, type=str, choices=['df', 'train_bert', 'test_bert'])\n",
    "parser.add_argument(\"-target_summary_sent\", default='abs', type=str)\n",
    "parser.add_argument(\"-n_cpus\", default='2', type=str)\n",
    "parser.add_argument(\"-file\", default=None, type=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(RAW_DATA_DIR1, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "l=list()\n",
    "i=0\n",
    "file=None\n",
    "file_list = os.listdir(RAW_DATA_DIR1)\n",
    "if file==None:\n",
    "    for now in file_list:\n",
    "        if not 'txt' in now:\n",
    "            continue\n",
    "        with open (f'{RAW_DATA_DIR1}/{now}', 'r', encoding='utf-8') as f:\n",
    "            i=i+1\n",
    "            get = re.split('\\.|\\?|\\!|\\/', f.read())\n",
    "            l.append({'id':i, 'article_original':get})\n",
    "else:\n",
    "    with open (f'{RAW_DATA_DIR1}/{now}', 'r', encoding='utf-8') as f:\n",
    "        i=i+1\n",
    "        get = re.split('\\.|\\?|\\!|\\/', f.read())\n",
    "        l.append({'id':i, 'article_original':get})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>article_original</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[안녕하십니까,  여러분들 모두가 지구 과학의 신이 되는 그날까지,  지구과학 가르...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[우리 지난 시간에 대륙 이동설 베게너가 주장했던 거죠,  대륙 이동설에 대해서 학...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[그리고 이렇게 대륙 이동설과 맨틀 대류설 그리고 해저 확장설에 이르기까지, \\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[그러면 다르게 말하면 어떻게 말할 수가 있냐 하면 지구상에서 일어나는 지진이나 화...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>[그럼 이제 본격적으로 판 구조론에 대해서 알아보기 전에 이 판의 정의에 대해서 살...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>[그래서 만났을 때 야,  판이 뭔데라고 물어보면 이 지각과 이 지각과 판의 개념을...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>[그럼 이 상태에서 본격적으로 판에 대한 이야기를 한번 해보도록 할 텐데요,  왼쪽...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>[그래서 연약권이 요런 부분이다라고 생각해 주시면 되겠는데 우리가 이 연약권을 대략...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>[그러면 연약권 같은 경우에는 말랑말랑해,  그래서 움직일 수 있어,  이렇게 말씀...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>[그림을 통해서 그거 파악해 주시면 되겠고 또 한 가지만 더 한 가지만 더 말씀을 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>[판의 구조,  1번 암석권 두께가 약 100km에 해당하는 단단한 암석으로 이뤄진...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>[밀도는 해양판이 대륙판보다 크다, \\n\\n\\n이거 굉장히 중요합니다,  뒤에서 이...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>[해령 같은 경우 우리가 지금 이제 배울 발산형 경계 같은 경우에 해령이나 그런 애...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                   article_original\n",
       "0    1  [안녕하십니까,  여러분들 모두가 지구 과학의 신이 되는 그날까지,  지구과학 가르...\n",
       "1    2  [우리 지난 시간에 대륙 이동설 베게너가 주장했던 거죠,  대륙 이동설에 대해서 학...\n",
       "2    3  [그리고 이렇게 대륙 이동설과 맨틀 대류설 그리고 해저 확장설에 이르기까지, \\n\\...\n",
       "3    4  [그러면 다르게 말하면 어떻게 말할 수가 있냐 하면 지구상에서 일어나는 지진이나 화...\n",
       "4    5  [그럼 이제 본격적으로 판 구조론에 대해서 알아보기 전에 이 판의 정의에 대해서 살...\n",
       "5    6  [그래서 만났을 때 야,  판이 뭔데라고 물어보면 이 지각과 이 지각과 판의 개념을...\n",
       "6    7  [그럼 이 상태에서 본격적으로 판에 대한 이야기를 한번 해보도록 할 텐데요,  왼쪽...\n",
       "7    8  [그래서 연약권이 요런 부분이다라고 생각해 주시면 되겠는데 우리가 이 연약권을 대략...\n",
       "8    9  [그러면 연약권 같은 경우에는 말랑말랑해,  그래서 움직일 수 있어,  이렇게 말씀...\n",
       "9   10  [그림을 통해서 그거 파악해 주시면 되겠고 또 한 가지만 더 한 가지만 더 말씀을 ...\n",
       "10  11  [판의 구조,  1번 암석권 두께가 약 100km에 해당하는 단단한 암석으로 이뤄진...\n",
       "11  12  [밀도는 해양판이 대륙판보다 크다, \\n\\n\\n이거 굉장히 중요합니다,  뒤에서 이...\n",
       "12  13  [해령 같은 경우 우리가 지금 이제 배울 발산형 경계 같은 경우에 해령이나 그런 애..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(l)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df(13) is exported\n"
     ]
    }
   ],
   "source": [
    "df.to_pickle(f'{RAW_DATA_DIR1}/ebs_pickle')\n",
    "print(f'df({len(df)}) is exported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text, tokenizer=None):\n",
    "    text = noise_remove(text)\n",
    "    if tokenizer is not None:\n",
    "        text = tokenizer(text)\n",
    "        text = ' '.join(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def korean_sent_spliter(doc):\n",
    "    sents_splited = kss.split_sentences(doc)\n",
    "    if len(sents_splited) == 1:\n",
    "        # .이나 ?가 있는데도 kss가 분리하지 않은 문장들을 혹시나해서 살펴보니\n",
    "        # 대부분 쉼표나 가운데점 대신 .을 사용하거나 \"\" 사이 인용문구 안에 들어가있는 점들. -> 괜찮.\n",
    "        # aa = sents_splited[0].split('. ')\n",
    "        # if len(aa) > 1:\n",
    "        #     print(sents_splited)\n",
    "        return sents_splited\n",
    "    else:  # kss로 분리가 된 경우(3문장 이상일 때도 고려)\n",
    "        #print(sents_splited)\n",
    "        for i in range(len(sents_splited) - 1):\n",
    "            idx = 0\n",
    "            # 두 문장 사이에 .이나 ?가 없는 경우: 그냥 붙여주기\n",
    "            if sents_splited[idx][-1] not in ['.','?' ] and idx < len(sents_splited) - 1:\n",
    "                sents_splited[idx] = sents_splited[idx] + ' ' + sents_splited[idx + 1] if doc[len(sents_splited[0])] == ' ' \\\n",
    "                                        else sents_splited[idx] + sents_splited[idx + 1] \n",
    "                del sents_splited[idx + 1]\n",
    "                idx -= 1\n",
    "        #print(sents_splited)\n",
    "        return sents_splited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_json_files(df, data_type='train', target_summary_sent=None, path=''):\n",
    "    NUM_DOCS_IN_ONE_FILE = 1000\n",
    "    start_idx_list = list(range(0, len(df), NUM_DOCS_IN_ONE_FILE))\n",
    "\n",
    "    for start_idx in tqdm(start_idx_list):\n",
    "        end_idx = start_idx + NUM_DOCS_IN_ONE_FILE\n",
    "        if end_idx > len(df):\n",
    "            end_idx = len(df)  # -1로 하니 안됨...\n",
    "\n",
    "        #정렬을 위해 앞에 0 채워주기\n",
    "        length = len(str(len(df)))\n",
    "        start_idx_str = (length - len(str(start_idx)))*'0' + str(start_idx)\n",
    "        end_idx_str = (length - len(str(end_idx-1)))*'0' + str(end_idx-1)\n",
    "\n",
    "        file_name = os.path.join(f'{path}/{data_type}_{target_summary_sent}' \\\n",
    "                                + f'/{data_type}.{start_idx_str}_{end_idx_str}.json') if target_summary_sent is not None \\\n",
    "                    else os.path.join(f'{path}/{data_type}' \\\n",
    "                                + f'/{data_type}.{start_idx_str}_{end_idx_str}.json')\n",
    "        \n",
    "        json_list = []\n",
    "        for i, row in df.iloc[start_idx:end_idx].iterrows():\n",
    "            original_sents_list = [preprocessing(original_sent).split()  # , korean_tokenizer\n",
    "                                    for original_sent in row['article_original']]\n",
    "\n",
    "            summary_sents_list = []\n",
    "            if target_summary_sent is not None:\n",
    "                if target_summary_sent == 'ext':\n",
    "                    summary_sents = row['extractive_sents']\n",
    "                elif target_summary_sent == 'abs':\n",
    "                    summary_sents = korean_sent_spliter(row['abstractive'])   \n",
    "                summary_sents_list = [preprocessing(original_sent).split() # , korean_tokenizer\n",
    "                                        for original_sent in summary_sents]\n",
    "\n",
    "            json_list.append({'src': original_sents_list,\n",
    "                              'tgt': summary_sents_list\n",
    "            })\n",
    "            #print(json_list)\n",
    "        #     break\n",
    "        # break\n",
    "        json_string = json.dumps(json_list, indent=4, ensure_ascii=False)\n",
    "        #print(json_string)\n",
    "        with open(file_name, 'w+') as json_file:\n",
    "            json_file.write(json_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_remove(text):\n",
    "    text = text.lower()\n",
    "    \n",
    "    # url 대체\n",
    "    # url_pattern = re.compile(r'https?://\\S*|www\\.\\S*')\n",
    "    # text = url_pattern.sub(r'URL', text)\n",
    "\n",
    "    # html 삭제\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    text = soup.get_text(separator=\" \")\n",
    "\n",
    "    # 숫자 중간에 공백 삽입하기\n",
    "    # text = number_split(text)\n",
    "    #number_pattern = re.compile('\\w*\\d\\w*') \n",
    "#     number_pattern = re.compile('\\d+') \n",
    "#     text = number_pattern.sub(r'[[NUMBER]]', text)\n",
    "    \n",
    "\n",
    "    # PUCTUACTION_TO_REMOVED = string.punctuation.translate(str.maketrans('', '', '\\\"\\'#$%&\\\\@'))  # !\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~ 중 적은것을 제외한 나머지를 삭제\n",
    "    # text = text.translate(str.maketrans(PUCTUACTION_TO_REMOVED, ' '*len(PUCTUACTION_TO_REMOVED))) \n",
    "\n",
    "    # remove_redundant_white_spaces\n",
    "    text = re.sub(' +', ' ', text)\n",
    "\n",
    "    # tgt special token 으로 활용할 204; 314[ 315] 대체/삭제해줘서 없애주기\n",
    "    text = re.sub('¶', ' ', text)\n",
    "    text = re.sub('----------------', ' ', text)\n",
    "    text = re.sub(';', '.', text)\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 42.15it/s]\n"
     ]
    }
   ],
   "source": [
    "bert_data_dir = f\"{BERT_DATA_DIR}/ebs_test_bert\"\n",
    "if os.path.exists(bert_data_dir):\n",
    "    os.system(f\"rm {bert_data_dir}/*\")\n",
    "else:\n",
    "    os.mkdir(bert_data_dir)\n",
    "\n",
    "json_data_dir = f\"{JSON_DATA_DIR}/ebs_test_script\"\n",
    "if os.path.exists(json_data_dir):\n",
    "        os.system(f\"rm {json_data_dir}/*\")\n",
    "else:\n",
    "    os.mkdir(json_data_dir)\n",
    "    \n",
    "create_json_files(df, data_type='ebs_test_script', path=JSON_DATA_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(f\"python preprocess.py\"\n",
    "    + f\" -mode format_to_bert -dataset ebs_test_script\"\n",
    "    + f\" -raw_path {json_data_dir}\"\n",
    "    + f\" -save_path {bert_data_dir}\"\n",
    "    + f\" -log_file {LOG_PREPO_FILE}\"\n",
    "    + f\" -lower -n_cpus 4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result_path=/home/ubuntu/KoBertSum/ext/test_ebs\n"
     ]
    }
   ],
   "source": [
    "test_file=f'{PROJECT_DIR}/ext/test_ebs/result_ebs_test_step_12000.candidate'\n",
    "if os.path.exists(test_file):\n",
    "    os.system(f'rm {test_file}')\n",
    "    \n",
    "print(f'result_path={PROJECT_DIR}/ext/test_ebs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(f\"python train.py -task ext -mode test \\\n",
    "            -test_from ../ext/models/0909_0929/model_step_12000.pt \\\n",
    "            -bert_data_path {BERT_DATA_DIR}/ebs_test_bert \\\n",
    "            -result_path {PROJECT_DIR}/ext/test_ebs \\\n",
    "            -log_file {PROJECT_DIR}/ext/logs/test_ebs.log \\\n",
    "            -test_batch_size 1  -batch_size 3000 \\\n",
    "            -sep_optim true -use_interval true -visible_gpus 0,1 \\\n",
    "            -max_pos 512 -max_length 200 -alpha 0.95 -min_length 50 \\\n",
    "            -report_rouge False \\\n",
    "            -max_tgt_len 100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
